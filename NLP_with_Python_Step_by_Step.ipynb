{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pankajit/DS-AI-ML/blob/master/NLP_with_Python_Step_by_Step.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcYaTxsuKfhW"
      },
      "source": [
        "# NLP with Python — Step by Step (Hands‑on)\n",
        "\n",
        "Welcome! This notebook teaches core Natural Language Processing (NLP) workflows **with practical Python code**.  \n",
        "We start simple (tokenization, n‑grams, TF‑IDF) and build up to **text classification**, **topic modeling**, **similarity search**, and (optional) **NER**.\n",
        "\n",
        "> Everything uses tiny in-notebook datasets so you can run it anywhere. Optional cells let you install libraries or try bigger models (spaCy / Transformers)."
      ],
      "id": "fcYaTxsuKfhW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TtkdPG7KfhY"
      },
      "source": [
        "## What you'll learn\n",
        "1. Text cleaning & tokenization (regex, stopwords)\n",
        "2. Features: **Bag of Words** & **TF‑IDF**\n",
        "3. **n‑grams** and why they help\n",
        "4. Build a **text classifier** (Logistic Regression) with scikit‑learn\n",
        "5. Evaluate: accuracy, confusion matrix, top features\n",
        "6. **Topic modeling** (LDA) for unsupervised themes\n",
        "7. **Cosine similarity** for search-like matching\n",
        "8. *(Optional)* **spaCy NER** and *(Optional)* **Transformers sentiment**"
      ],
      "id": "0TtkdPG7KfhY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33SOUJxKKfhZ"
      },
      "source": [
        "## Quick Setup (optional)\n",
        "Run this block if you're in a fresh environment (e.g., Colab) to install dependencies."
      ],
      "id": "33SOUJxKKfhZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsHGkyRJKfha",
        "outputId": "c35b6500-eabf-4bb9-e608-a90dceb13a9d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing packages...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Setup complete.\n"
          ]
        }
      ],
      "source": [
        "# OPTIONAL: install common NLP libs\n",
        "# Remove the leading '!' if your environment doesn't allow shell commands.\n",
        "# You can skip this if you already have these installed.\n",
        "try:\n",
        "    import sklearn, nltk, gensim, spacy, matplotlib\n",
        "except Exception as e:\n",
        "    print(\"Installing packages...\")\n",
        "    # Comment out any you don't want\n",
        "    !pip -q install scikit-learn nltk gensim spacy matplotlib seaborn\n",
        "    # Light model for spaCy NER (optional)\n",
        "    !python -m spacy download en_core_web_sm -q\n",
        "\n",
        "import nltk\n",
        "# Download lightweight NLTK resources (safe to re-run)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "\n",
        "print(\"Setup complete.\")"
      ],
      "id": "KsHGkyRJKfha"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1fyjMAAKfhb"
      },
      "source": [
        "## 1) Tiny Labeled Dataset (Sentiment)\n",
        "We'll create a small toy dataset with **positive** and **negative** sentiments."
      ],
      "id": "W1fyjMAAKfhb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJded09aKfhb",
        "outputId": "5c2f9b45-8432-4ae1-d607-784a347da490"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['pos', 'pos', 'pos', 'pos', 'pos', 'neg', 'neg', 'neg', 'neg', 'neg']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,\n",
              " ['I love this phone, the camera is amazing and battery lasts all day',\n",
              "  'Absolutely fantastic build quality and performance'],\n",
              " ['pos', 'pos'])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "toy_data = [\n",
        "    (\"I love this phone, the camera is amazing and battery lasts all day\", \"pos\"),\n",
        "    (\"Absolutely fantastic build quality and performance\", \"pos\"),\n",
        "    (\"Best purchase I've made this year, super happy!\", \"pos\"),\n",
        "    (\"The service was quick and friendly, highly recommend\", \"pos\"),\n",
        "    (\"Great value for money, works like a charm\", \"pos\"),\n",
        "    (\"Terrible experience, the product broke in two days\", \"neg\"),\n",
        "    (\"Worst customer support, very rude and unhelpful\", \"neg\"),\n",
        "    (\"I hate the design and the screen is awful\", \"neg\"),\n",
        "    (\"Total waste of money, not worth it\", \"neg\"),\n",
        "    (\"It arrived damaged and the return process is painful\", \"neg\"),\n",
        "]\n",
        "\n",
        "texts = [t for t, _ in toy_data]\n",
        "labels = [y for _, y in toy_data]\n",
        "len(texts), texts[:2], labels[:2]"
      ],
      "id": "uJded09aKfhb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6aHdtJ4Kfhc"
      },
      "source": [
        "## 2) Basic Preprocessing\n",
        "We will:\n",
        "- lowercase text\n",
        "- remove punctuation\n",
        "- tokenize\n",
        "- remove stopwords\n",
        "- *(optional)* lemmatize"
      ],
      "id": "c6aHdtJ4Kfhc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpl-XGPTKfhc"
      },
      "execution_count": 9,
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "STOPWORDS = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_tokenize(text, do_lemma=False):\n",
        "    # lowercase\n",
        "    text = text.lower()\n",
        "    # remove punctuation (keep letters + spaces)\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
        "    # tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "    # remove stopwords and short tokens\n",
        "    tokens = [t for t in tokens if t not in STOPWORDS and len(t) > 2]\n",
        "    if do_lemma:\n",
        "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return tokens\n",
        "\n",
        "    print(clean_tokenize(texts[0], do_lemma=True))"
      ],
      "id": "Qpl-XGPTKfhc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEYFDKqtKfhc"
      },
      "source": [
        "## 3) Bag of Words (BoW) & TF‑IDF\n",
        "We'll vectorize texts using **CountVectorizer** (BoW) and **TfidfVectorizer**."
      ],
      "id": "YEYFDKqtKfhc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "6YrMfJziKfhd",
        "outputId": "70a356e0-951b-478d-f9eb-e183d81f094a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-974826507.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# We'll provide our own tokenizer to apply the same cleaning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean_tokenize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BoW shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_bow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample features:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1374\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2459677602.py\u001b[0m in \u001b[0;36mclean_tokenize\u001b[0;34m(text, do_lemma)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"[^a-z\\s]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m# remove stopwords and short tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSTOPWORDS\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# We'll provide our own tokenizer to apply the same cleaning\n",
        "cv = CountVectorizer(analyzer=clean_tokenize)\n",
        "X_bow = cv.fit_transform(texts)\n",
        "print(\"BoW shape:\", X_bow.shape)\n",
        "print(\"Sample features:\", list(cv.vocabulary_.keys())[:20])\n",
        "\n",
        "tfidf = TfidfVectorizer(analyzer=clean_tokenize)\n",
        "X_tfidf = tfidf.fit_transform(texts)\n",
        "print(\"TF-IDF shape:\", X_tfidf.shape)"
      ],
      "id": "6YrMfJziKfhd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5MBO2FLKfhd"
      },
      "source": [
        "## 4) n‑grams\n",
        "Let’s capture bigrams/trigrams to encode short phrases."
      ],
      "id": "d5MBO2FLKfhd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Bm1SLaMKfhd"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "tfidf_ngrams = TfidfVectorizer(analyzer=clean_tokenize, ngram_range=(1,2), min_df=1)\n",
        "X_tfidf_ngrams = tfidf_ngrams.fit_transform(texts)\n",
        "print(\"TF-IDF with unigrams+bigrams:\", X_tfidf_ngrams.shape)\n",
        "# Show top 15 features by IDF (lowest df = most rare) just to peek\n",
        "features = tfidf_ngrams.get_feature_names_out()\n",
        "print(\"Example features:\", features[:30])"
      ],
      "id": "-Bm1SLaMKfhd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ1CAcHSKfhd"
      },
      "source": [
        "## 5) Text Classification (Logistic Regression)\n",
        "We'll train a simple classifier on the toy dataset using **TF‑IDF (1-2 grams)**."
      ],
      "id": "IZ1CAcHSKfhd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkvvruYrKfhe"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf_ngrams, labels, test_size=0.3, random_state=42, stratify=labels)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, pred))\n",
        "print(\"\\nReport:\\n\", classification_report(y_test, pred))"
      ],
      "id": "GkvvruYrKfhe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "737bbDy1Kfhe"
      },
      "source": [
        "## 6) Inspect Important Features\n",
        "Which words/phrases push the model toward **positive** vs **negative**?"
      ],
      "id": "737bbDy1Kfhe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fb6iwkg2Kfhe"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "feature_names = tfidf_ngrams.get_feature_names_out()\n",
        "coefs = clf.coef_[0]  # Binary classifier -> single row\n",
        "top_pos_idx = np.argsort(coefs)[-10:][::-1]\n",
        "top_neg_idx = np.argsort(coefs)[:10]\n",
        "\n",
        "print(\"Top POSITIVE indicators:\")\n",
        "for i in top_pos_idx:\n",
        "    print(f\"{feature_names[i]:25s}  {coefs[i]: .3f}\")\n",
        "\n",
        "print(\"\\nTop NEGATIVE indicators:\")\n",
        "for i in top_neg_idx:\n",
        "    print(f\"{feature_names[i]:25s}  {coefs[i]: .3f}\")"
      ],
      "id": "Fb6iwkg2Kfhe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JFpOgv1Kfhe"
      },
      "source": [
        "## 7) Topic Modeling (LDA)\n",
        "Unsupervised discovery of themes with **Latent Dirichlet Allocation**.\n",
        "We'll use a small corpus of product reviews and tech sentences."
      ],
      "id": "2JFpOgv1Kfhe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hluVXULjKfhe"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "extra_corpus = [\n",
        "    \"The laptop performance is great for programming and data analysis\",\n",
        "    \"Battery life could be better but the keyboard is comfortable\",\n",
        "    \"I love the new camera features and image stabilization\",\n",
        "    \"Customer service resolved my issue quickly and professionally\",\n",
        "    \"Hate the lag and random crashes after the latest update\",\n",
        "    \"Docker and Kubernetes help scale microservices in production\",\n",
        "    \"Neural networks excel at image and text classification\",\n",
        "    \"Cloud storage redundancy prevents accidental data loss\",\n",
        "    \"The display is crisp, but speakers are too quiet\",\n",
        "    \"Refund process was smooth and the agent was polite\"\n",
        "]\n",
        "\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "\n",
        "lda_vec = CountVectorizer(analyzer=clean_tokenize, min_df=1)\n",
        "X_lda = lda_vec.fit_transform(extra_corpus)\n",
        "lda = LDA(n_components=2, random_state=42, learning_method=\"batch\")\n",
        "lda.fit(X_lda)\n",
        "\n",
        "words = np.array(lda_vec.get_feature_names_out())\n",
        "for topic_idx, comp in enumerate(lda.components_):\n",
        "    top_idx = np.argsort(comp)[-10:][::-1]\n",
        "    print(f\"\\nTopic {topic_idx}:\")\n",
        "    print(\", \".join(words[top_idx]))"
      ],
      "id": "hluVXULjKfhe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHMC87xbKfhe"
      },
      "source": [
        "## 8) Text Similarity (Cosine)\n",
        "Build a simple search: given a **query**, retrieve the most similar sentences."
      ],
      "id": "VHMC87xbKfhe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4fTfUMAKfhf"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Use TF-IDF on the extra_corpus\n",
        "tfidf_sim = TfidfVectorizer(analyzer=clean_tokenize, ngram_range=(1,2))\n",
        "X_sim = tfidf_sim.fit_transform(extra_corpus)\n",
        "\n",
        "def search(query, top_k=3):\n",
        "    q = tfidf_sim.transform([query])\n",
        "    scores = cosine_similarity(q, X_sim).flatten()\n",
        "    best_idx = np.argsort(scores)[-top_k:][::-1]\n",
        "    return [(extra_corpus[i], float(scores[i])) for i in best_idx]\n",
        "\n",
        "for q in [\"camera stabilization\", \"customer support\", \"cloud production\", \"neural networks\"]:\n",
        "    print(f\"\\nQuery: {q}\")\n",
        "    for sent, sc in search(q):\n",
        "        print(f\"  -> ({sc:.3f}) {sent}\")"
      ],
      "id": "Y4fTfUMAKfhf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5tHmpSHKfhf"
      },
      "source": [
        "## 9) (Optional) Named Entity Recognition with spaCy\n",
        "Try extracting **people, orgs, locations** automatically.  \n",
        "This cell uses the small English model; if it's not available, the install step above adds it."
      ],
      "id": "i5tHmpSHKfhf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onRWjXRrKfhf"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "try:\n",
        "    import spacy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    sample = \"Apple is opening a new office in Bengaluru and hiring 500 engineers in 2025.\"\n",
        "    doc = nlp(sample)\n",
        "    print([(ent.text, ent.label_) for ent in doc.ents])\n",
        "except Exception as e:\n",
        "    print(\"spaCy or model not installed. Run the setup cell above if you want to try NER.\")\n",
        "    print(\"Error:\", e)"
      ],
      "id": "onRWjXRrKfhf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHXOa1jKKfhf"
      },
      "source": [
        "## 10) (Optional) Transformers (Hugging Face)\n",
        "Quick demo using a pre-trained sentiment pipeline.  \n",
        "> This downloads a small model at runtime; skip if you're offline."
      ],
      "id": "BHXOa1jKKfhf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_s-XbyxvKfhf"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "try:\n",
        "    from transformers import pipeline\n",
        "    clf_pipe = pipeline('sentiment-analysis')\n",
        "    print(clf_pipe(\"I absolutely love this phone!\"))\n",
        "    print(clf_pipe(\"This is the worst update ever.\"))\n",
        "except Exception as e:\n",
        "    print(\"Transformers not available (or no internet). You can install with:\")\n",
        "    print(\"!pip install transformers torch --quiet\")\n",
        "    print(\"Error:\", e)"
      ],
      "id": "_s-XbyxvKfhf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRatzr0XKfhf"
      },
      "source": [
        "## 11) Mini‑Project: Sentiment Classifier Function\n",
        "A small utility you can re-use. It trains on our toy set and predicts on new text."
      ],
      "id": "CRatzr0XKfhf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qc4bpni2Kfhf"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    (\"tfidf\", TfidfVectorizer(analyzer=clean_tokenize, ngram_range=(1,2))),\n",
        "    (\"lr\", LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "pipeline.fit(texts, labels)\n",
        "\n",
        "def predict_sentiment(s: str):\n",
        "    return pipeline.predict([s])[0], pipeline.predict_proba([s])[0].max()\n",
        "\n",
        "tests = [\n",
        "    \"I am delighted with the new features and the speed\",\n",
        "    \"Horrible bug, app keeps crashing and support ignores me\",\n",
        "]\n",
        "for t in tests:\n",
        "    label, conf = predict_sentiment(t)\n",
        "    print(f\"{t} -> {label} ({conf:.2f})\")"
      ],
      "id": "qc4bpni2Kfhf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaM-qIRqKfhg"
      },
      "source": [
        "## Next Steps\n",
        "- Replace the toy dataset with your real data (CSV), wrap pipelines in functions\n",
        "- Try cross-validation (e.g., `StratifiedKFold`)\n",
        "- Clean text more (URLs, emojis), add **char-level n‑grams** for misspellings\n",
        "- Use **GridSearchCV** to tune hyperparameters\n",
        "- Try **fastText**, **GloVe**, or **Transformers** for stronger accuracy\n",
        "- Move to **spaCy** for production pipelines (tokenization, NER, POS)\n",
        "- Explore **LangChain / RAG** for QA over your documents"
      ],
      "id": "GaM-qIRqKfhg"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}